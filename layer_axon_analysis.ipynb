{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### March 2023\n",
    "Newest axon analysis procedures for layer specific project\n",
    "\n",
    "Cleaned up code from to_lucas.ipynb with detailed comments\n",
    "\n",
    "April update:\n",
    "added normalization for tiff stack and excel output\n",
    "Divide every item by the total number of axons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "import warnings\n",
    "\n",
    "import tkinter.filedialog as fdialog\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import Neuron_analysis as na\n",
    "from Neuron_analysis import *\n",
    "import skimage\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from allensdk.core.mouse_connectivity_cache import MouseConnectivityCache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_pt_natlas(dspoint_name, to_add):\n",
    "    '''Read downsampled points after Transformix transformation that are now in Allen Atlas domain, then add the missing y sections that was cropped off previously during the registration step\n",
    "    Feb 2022\n",
    "        Modified for viral images where points are indices after trailmap segmentation with probability \n",
    "        Since some noise will be picked up as axons and cross probability threshold, it is possible for transformed points to have negative value or outside of the brain\n",
    "        points with negative values any one dimensions are ignored\n",
    "    Apr 2022\n",
    "        Modified to not include atlas name\n",
    "        Prefer to select atlas directly\n",
    "    '''\n",
    "    with open(dspoint_name,'r') as output:\n",
    "        outputpoint= output.readlines()\n",
    "    \n",
    "    all_points=[]\n",
    "\n",
    "    for lines in tqdm(outputpoint):\n",
    "        m=re.search(\"(?:OutputIndexFixed = \\[ )([0-9]+ [0-9]+ [0-9]+)\", lines)\n",
    "        if not m:\n",
    "            #print('negative number in one of the dimension, skipped')\n",
    "            #print(f'{lines}')\n",
    "            pass\n",
    "        else:\n",
    "            m=m.groups(0)\n",
    "            this_line= str(m[0]).split(' ')\n",
    "            mypoints= [int(stuff) for stuff in this_line]\n",
    "            mypoints[1]= mypoints[1]+to_add\n",
    "            all_points.append(mypoints)\n",
    "    \n",
    "    return all_points\n",
    "\n",
    "def find_points_id(points):\n",
    "    \n",
    "    '''find region id for each coordinate, fix some of noticable errors in allen atlas and potential contribution of misalignment'''\n",
    "\n",
    "    # for some brains (ie.AL215) because somedownsampled points are located beyond the altas (??), temporarily assign an index of 0\n",
    "    # this keeps the completeness for later use (ie, propagating region id to original points)\n",
    "    points_in_atlas=[]\n",
    "    \n",
    "    for i in points:\n",
    "        if i[0]<456 and i[1]<528 and i[2]<320:\n",
    "            points_in_atlas.append(int(annot_h[i[0], i[1],i[2]]))\n",
    "        # check if the point index is smaller or equalt to the atlas\n",
    "        else:\n",
    "            points_in_atlas.append(0)\n",
    "\n",
    "    points_in_atlas= np.where(points_in_atlas==484682520, 484682528 , points_in_atlas) \n",
    "    points_in_atlas= np.where(points_in_atlas==484682524, 484682528 , points_in_atlas)\n",
    "    # replace id= 484682520 (optic radiation)  and id= 484682524 (auditory radiation) with 484682258, stc(a subregion of fiber bundle)\n",
    "    # these are intrinsic issue of the allen atlas, the labels for these regions are wrong\n",
    "    \n",
    "    points_in_atlas= np.where(points_in_atlas==382, 466 , points_in_atlas)\n",
    "    # replace id= 382, hippocampus CA1 with 466, alveus(a subregion of fornix system), to correct for slight misalignment with atlas\n",
    "    \n",
    "    return points_in_atlas\n",
    "\n",
    "def make_tif(all_points, atlas_shape):\n",
    "    ''' Project downsampled points on to a tif stack, useful for overlaping with brain or template (ie, in imageJ)\n",
    "    input: downsampled points in a list containing x y z ordinates as int, directory containing it (this is also the output directory) and whether annotation is axon or not (default True)\n",
    "    example: [[12, 13, 25],\n",
    "             [13, 14, 25],...]\n",
    "    \n",
    "    output: a numpy array with the same dimensions of the template/atlas mhd files with downsampled points only\n",
    "    each point has a value of the number of occurences (since downsampling combines multiple points as one)\n",
    "    \n",
    "    april 2023 update: now outputs an numpy array instead of directly saved as tiff \n",
    "    '''\n",
    "    \n",
    "    svolume=np.zeros(atlas_shape)\n",
    "    #columns, rows, planes\n",
    "\n",
    "    zplanes=[]\n",
    "    for i in all_points:\n",
    "        zplanes.append( i[2])\n",
    "    zplanes=np.unique(zplanes)\n",
    "    temp=np.zeros(atlas_shape[0:2])\n",
    "    thepoints=np.asarray(all_points)\n",
    "    \n",
    "    real_zplanes= zplanes[zplanes <atlas_shape[2]] # get rid of everything beyond the shape of atlas\n",
    "\n",
    "    for i in tqdm(real_zplanes):\n",
    "        index= thepoints[:,2]==i\n",
    "        uindex,counts=np.unique(thepoints[index],return_counts=True, axis=0)\n",
    "        for j, lines in enumerate(uindex):\n",
    "            coord1,coord2=lines[0:2]\n",
    "            temp[coord1][coord2]= counts[j]\n",
    "        svolume[:,:,i]=temp #write this in \n",
    "        temp=np.zeros(atlas_shape[0:2]) #reset the empty plane after each z\n",
    "\n",
    "\n",
    "    horizontal_planetmp= np.swapaxes(np.int16(svolume),0,2)\n",
    "    #for some reason, if just save stuff as tiff, it will save x planes of yz view\n",
    "    #here we shift the 3rd dimension with the first dimension to obtain xy view\n",
    "    \n",
    "    return horizontal_planetmp\n",
    "\n",
    "def save_tiff(numpystack, outname):\n",
    "    \n",
    "    '''save numpy stack as tiff'''\n",
    "    \n",
    "    print('Starting to saving tif files..')\n",
    "    \n",
    "    io.imsave(outname+'_axons.tif',numpystack)\n",
    "    \n",
    "    sum_0=np.sum(numpystack, axis=0)\n",
    "    sum_1=np.sum(numpystack, axis=1)\n",
    "    sum_2=np.sum(numpystack, axis=2)\n",
    "    io.imsave(outname+ '_sum_0.tif',sum_0)\n",
    "    io.imsave(outname+ '_sum_1.tif',sum_1)\n",
    "    io.imsave(outname+ '_sum_2.tif',sum_2)\n",
    "\n",
    "def regions_csv(points_in_atlas,total_axons, out_name):\n",
    "    \n",
    "    '''Saves brain region info and counts in excel file.\n",
    "    Note that coordinates with an brain index of 0 will not get included because it does not have a place in the atlas labels.csv'''\n",
    "\n",
    "    unique_id, counts = np.unique(points_in_atlas, return_counts=True)\n",
    "    counts=counts/total_axons #normalize by total number of axon points\n",
    "    id_withcounts=list(zip(unique_id, counts))\n",
    "\n",
    "    our_regions=na.atlas_labels.loc[na.atlas_labels['id'].isin (unique_id)]\n",
    "\n",
    "    new_df= pd.DataFrame(id_withcounts, columns=['id', 'counts'])\n",
    "    our_regionWcounts=pd.merge(na.atlas_labels, new_df)\n",
    "    \n",
    "    our_regionWcounts.to_excel(out_name+'region_with_counts.xlsx',index=None,header=True)\n",
    "\n",
    "    return our_regionWcounts\n",
    "\n",
    "def parent_df(df):\n",
    "    # group dataframe by parent id structure\n",
    "    grouped_pd=df.groupby(['parent_structure_id'],as_index=False).sum()\n",
    "    d= {'id': grouped_pd.parent_structure_id.astype(int), 'counts': grouped_pd.counts}\n",
    "    grouped_pd2= pd.DataFrame(data=d)\n",
    "    result = pd.merge(grouped_pd2, na.atlas_labels, on=[\"id\"])\n",
    "    result.sort_values(['counts'], ascending=True, inplace=True)\n",
    "    # result is the final pd\n",
    "\n",
    "    return result\n",
    "\n",
    "def clean_duplicate(df):\n",
    "    '''This is needed again to account for parent regions that have its subregion incompletely covers the parent areas.\n",
    "    A painful example is Zona incerta, since it has a depth of 6, after group by parents it will show up twice with different counts where we simply just add the two counts'''\n",
    "    \n",
    "    df2=df.groupby(['acronym'],as_index=False, sort=False).sum()\n",
    "    d= {'acronym': df2.acronym, 'counts': df2.counts}\n",
    "    grouped_pd2= pd.DataFrame(data=d)\n",
    "    result = pd.merge(grouped_pd2, na.atlas_labels, on=[\"acronym\"])\n",
    "    # this merging is required because pd.groupby will drop 'useless columns' such as depth, structure id path and other useful ones!! So we fetch them back here..\n",
    "    # Probably have better ways of doing it...\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronal atlas has shape (528, 320, 456)\n",
      "Converted to horizontal atlas with shape (456, 528, 320)\n"
     ]
    }
   ],
   "source": [
    "mcc = MouseConnectivityCache(resolution=25)\n",
    "annot, annot_info = mcc.get_annotation_volume()\n",
    "print('Coronal atlas has shape', annot.shape)\n",
    "# load allen mouse brain atlas that is in coronal view\n",
    "\n",
    "annot_h=np.moveaxis(annot, 2, 0)\n",
    "print('Converted to horizontal atlas with shape', annot_h.shape)\n",
    "# reslice corontal atlas to horizontal atlas that is consistent with our sample's orientation\n",
    "\n",
    "atlas_labels=pd.read_csv('D:\\Allenbrainatlas\\ARA_25_micron_mhd_ccf2017\\labels.csv')\n",
    "# read axon labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing AL330\n"
     ]
    }
   ],
   "source": [
    "points_name=fdialog.askopenfile( title='Select the downsampled transformed axon points').name\n",
    "# specify the name of transformed axon points after transform_points.py\n",
    "\n",
    "mouse_name= na.find_mousename(points_name)\n",
    "# identify mouse id\n",
    "\n",
    "indir = f'D:\\\\Viral_stacks\\\\{mouse_name}\\\\'\n",
    "# find directory that contains the cropped template in order to refill the missing y sections\n",
    "\n",
    "outdir =r'D:\\viral_results'\n",
    "# define folder to store the results\n",
    "\n",
    "out_name= os.path.join(outdir,mouse_name)\n",
    "#define a generic output file name for this sample\n",
    "\n",
    "print(f'now processing {mouse_name}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refilling the leading 67 y section for each point\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 577967/577967 [00:02<00:00, 194909.12it/s]\n"
     ]
    }
   ],
   "source": [
    "template_name= [i for i in os.listdir(indir) if 'template_' in i]\n",
    "lead= na.find_crop(template_name[0])\n",
    "# finds the y section to refil from template name\n",
    "\n",
    "print (f'refilling the leading {lead} y section for each point')\n",
    "all_points=get_pt_natlas(points_name,lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# now find id for all points first (for future use of associating id to original non-downsample non-transformed points), this is saved as csv file during clean_points function\n",
    "points_in_atlas_all= find_points_id(all_points) \n",
    "\n",
    "d={'coordinates': all_points, 'id': points_in_atlas_all}\n",
    "new_df =pd.DataFrame(data=d)\n",
    "# store the corrdinates and their corresponding region id in a pandas dataframe\n",
    "\n",
    "#all_points_clean=new_df.coordinates.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "real_axons_tmp=new_df[new_df.id!=0]\n",
    "real_axons= real_axons_tmp.coordinates.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:01<00:00, 105.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to saving tif files..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:141: UserWarning:\n",
      "\n",
      "D:\\viral_results\\AL330_axons.tif is a low contrast image\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tiff_stack= make_tif(real_axons,annot_h.shape)\n",
    "norm_stack= np.divide(tiff_stack,len(real_axons))\n",
    "\n",
    "save_tiff(norm_stack, out_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "total_axons=len(real_axons)\n",
    "\n",
    "left_points= []\n",
    "right_points= []\n",
    "for item in all_points:\n",
    "    if item[0]<227:\n",
    "        left_points.append(item)\n",
    "    else:\n",
    "        right_points.append(item)    \n",
    "\n",
    "# splits points down the middle in to two lists into left and right hemisphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "points_in_atlasR= find_points_id(right_points)\n",
    "regionsR= regions_csv(points_in_atlasR, total_axons, out_name+'_axon_right_')\n",
    "\n",
    "points_in_atlasL= find_points_id(left_points)\n",
    "regionsL= regions_csv(points_in_atlasL, total_axons, out_name+'_axon_left_')\n",
    "# save brain region info and point counts into two separate excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate excel file of regions in parent name (1 graph order up) for left hemisphere\n",
    "\n",
    "axon_subL=regionsL.sort_values(by=['counts'])\n",
    "axon_subL.sort_values(by= 'graph_order',axis=0, inplace=True)\n",
    "\n",
    "needsgroup=axon_subL[axon_subL.depth>6]\n",
    "noneedsgroup=axon_subL[axon_subL.depth<=6]\n",
    "\n",
    "parentL= parent_df(needsgroup)\n",
    "completeL1=noneedsgroup.append(parentL)\n",
    "\n",
    "completeL=clean_duplicate(completeL1)\n",
    "# sums up counts from duplicated region, see an example inside function\n",
    "\n",
    "completeL.sort_values('counts',ascending=False, inplace=True)\n",
    "\n",
    "completeL.to_excel(f'{out_name}_Lparent.xlsx') \n",
    "print('left_parent excel saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Generate excel file of regions in parent name (1 graph order up) for right hemisphere\n",
    "\n",
    "if len(regionsR) == 0:\n",
    "    # still writes an empty file even if there are no axons in the right hemisphere!\n",
    "    # for completion purposes and later group heatmap\n",
    "    \n",
    "    regionsR.to_excel(f'{out_name}_Rparent.xlsx') \n",
    "    \n",
    "else:\n",
    "    axon_subR=regionsR.sort_values(by=['counts'])\n",
    "    axon_subR.sort_values(by= 'graph_order',axis=0, inplace=True)\n",
    "\n",
    "    needsgroupR=axon_subR[axon_subR.depth>6]\n",
    "    noneedsgroupR=axon_subR[axon_subR.depth<=6]\n",
    "\n",
    "    parentR= parent_df(needsgroupR)\n",
    "\n",
    "    completeR1=noneedsgroupR.append(parentR)\n",
    "\n",
    "    completeR=clean_duplicate(completeR1)\n",
    "\n",
    "    completeR.sort_values('counts',ascending=False, inplace=True)\n",
    "\n",
    "    completeR.to_excel(f'{out_name}_Rparent.xlsx') \n",
    "print('right_parent excel saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def arrange_parent_subregion(parentdf,subregiondf):\n",
    "    print('Re-arranged subregion order based on parent region that has the greatest number of axons...')\n",
    "    \n",
    "    new_order=parentdf.id.to_numpy()\n",
    "    \n",
    "    old_order= subregiondf.parent_structure_id.to_numpy()   \n",
    "    \n",
    "    #print('Old parent region id order is: ', old_order)\n",
    "    #print('New parent region id order is: ', new_order)\n",
    "    \n",
    "    \n",
    "    new_array= np.zeros_like(old_order)\n",
    "    for i, j in enumerate(new_order):\n",
    "        new_array[old_order==j]=i\n",
    "        \n",
    "    #print('Re-arranged subregion order based on parent region that has the greatest number of axon: ', new_array)\n",
    "    \n",
    "    subregiondf['new_order']= new_array\n",
    "    subregiondf.sort_values('new_order', inplace=True)\n",
    "    \n",
    "    subregiondf.sort_values(by=['new_order', 'graph_order'], ascending=[True, True] ,inplace=True)\n",
    "\n",
    "    return subregiondf\n",
    "\n",
    "def plot_hist(pd_axonL,pd_axonR, out_name):\n",
    "    ''' \n",
    "    Plot horizontal histogram of all points and ending points of axons and dendrites\n",
    "    Input: pandas dataframe of axon, pandas dataframe of dendrite, mousename\n",
    "    '''\n",
    "\n",
    "    fig = make_subplots(\n",
    "        shared_yaxes=True,\n",
    "        rows=2, cols=1,\n",
    "        row_heights=[0.6, 0.5],\n",
    "        row_titles=['Left axons', 'Right axons', 'Dendrites']\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "        y=pd_axonL['acronym'], \n",
    "        #x=pd_axonL['intensity'],\n",
    "        x=pd_axonL['counts']/1000, # units now in milimeters\n",
    "        marker_color='red', #for future, pd_axon['region_id'],\n",
    "        name='',\n",
    "        text=pd_axonL['name'],\n",
    "        hovertemplate=\n",
    "            '<i>%{x}</i>, '+\n",
    "            '<b>%{text}</b>',\n",
    "        orientation='h'),\n",
    "        row=1,col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "        y=pd_axonR['acronym'], \n",
    "        #x=pd_axonR['intensity'],\n",
    "        x=pd_axonR['counts']/1000, # units now in milimeters\n",
    "        marker_color='magenta', #for future, pd_axon['region_id'],\n",
    "        name='',\n",
    "        text=pd_axonR['name'],\n",
    "        hovertemplate=\n",
    "            '<i>%{x}</i>, '+\n",
    "            '<b>%{text}</b>',\n",
    "        orientation='h'),\n",
    "        row=2,col=1\n",
    "    )\n",
    "    \n",
    "\n",
    "   \n",
    "    fig.update_layout(yaxis={'categoryorder':'trace'}, \n",
    "                      width=2000,\n",
    "                      height=1000, # 1500 for AL066 since too many items\n",
    "                      showlegend= False,\n",
    "                      paper_bgcolor='rgba(0,0,0,0)', # transparent background\n",
    "                      plot_bgcolor='rgba(0,0,0,0)' # transparent background\n",
    "                     )\n",
    "    \n",
    "    fig.update_xaxes(gridcolor='gold')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "    fig.write_image(f\"{out_name}.svg\")\n",
    "    fig.write_html(f\"{out_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('For left hemisphere')\n",
    "new_axonL= arrange_parent_subregion(completeL,axon_subL)\n",
    "\n",
    "\n",
    "print('For right hemisphere')\n",
    "if len(regionsR) == 0:\n",
    "    new_axonR= regionsR\n",
    "    print('There are no axons in the right hemisphere')\n",
    "else:\n",
    "    new_axonR= arrange_parent_subregion(completeR,axon_subR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(new_axonL,new_axonR,out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
